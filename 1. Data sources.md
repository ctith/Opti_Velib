# Data Sources

## Récupération des données

### Données de Vélib entre 2015 et 2018 
**Site web** : https://nipil.org/jcdecaux_history_api_exports/

**Script python** : [dataBikePython.py](https://github.com/ctith/Projet_Velib/blob/master/dataBikeParis.py)
```python
import requests
from bs4 import BeautifulSoup

# structure de la page html
url = "https://nipil.org/jcdecaux_history_api_exports/"

"""
<html>
<head><title>Index of /jcdecaux_history_api_exports/</title></head>
<body bgcolor="white">
<h1>Index of /jcdecaux_history_api_exports/</h1><hr><pre><a href="../">../</a>
<a href="contracts.sql.bz2">contracts.sql.bz2</a>                                  26-Mar-2018 00:05                1264
<a href="rss.xml">rss.xml</a>                                            26-Mar-2018 00:05                3824
<a href="samples_2015_12_05.sql.bz2">samples_2015_12_05.sql.bz2</a>                         06-Dec-2015 07:59              311255
...
"""

# Recupere reponse HTTP GET du site
r = requests.get(url)

# verification du code renvoye par le server
def statusServer(status):
    switcher = {
        200 : "succes de la requete",
        301 : "redirection permanente",
        302 : "redirection temporaire",
        401 : "utilisateur non authentifie",
        403 : "acces refuse",
        404 : "page non trouvee",
        500 : "erreur serveur",
        503 : "erreur serveur",
        504 : "le serveur n'a pas repondu"
    }
    print "Le serveur nous renvoie le code {} = ".format(r.status_code, url) + switcher.get(status, "erreur inconnue")

statusServer(r.status_code)

# fichier output liens a telecharger
f = open("data.txt", "w")

# transformer html en dom
dom = BeautifulSoup(r.text, "html.parser")

# recuperer ligne html qui nous interesse
liensHTML = dom.find_all("a")

# recuperer info dans le bloc html qui nous interesse
for liens in liensHTML:
    liensCourts = liens.get('href')
    liensLongs = "https://nipil.org/jcdecaux_history_api_exports/"+liensCourts+'\n'
    f.write(liensLongs)
    print(liensLongs)

f.close()
```

**WGET**
```shell
wget ‐‐input data.txt 
```

## Analyse des données
```SQL
CREATE TABLE contracts (
    contract_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,
    timestamp INTEGER NOT NULL,
    contract_name TEXT UNIQUE NOT NULL,
    commercial_name TEXT NOT NULL,
    country_code TEXT NOT NULL,
    cities TEXT NOT NULL);
 ```                    
 
```SQL
CREATE TABLE stations (
	contract_id INTEGER NOT NULL,
	station_number INTEGR NOT NULL,
	status INTEGER NOT NULL,
	bike_stands INTEGER NOT NULL,
	bonus INTEGER NOT NULL,
	banking INTEGER NOT NULL,
	position TEXT NOT NULL,
	address TEXT NOT NULL,
	station_name TEXT NOT NULL,
	PRIMARY KEY (contract_id, station_number));
 ```
 
 ```SQL
 CREATE TABLE archived_samples (
    timestamp INTEGER NOT NULL,
    contract_id INTEGER NOT NULL,
    station_number INTEGR NOT NULL,
    available_bikes INTEGER NOT NULL,
    available_bike_stands INTEGER NOT NULL,
    PRIMARY KEY (timestamp, contract_id, station_number)
) WITHOUT ROWID;
 ```
